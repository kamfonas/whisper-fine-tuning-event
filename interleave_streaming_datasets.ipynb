{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4f1f016",
   "metadata": {},
   "source": [
    "# Interleaving Datasets\n",
    "\n",
    "Original: sanchit-gandhi \\\n",
    "This version: Michael Kamfonas (Farsipal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5c0357",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Ensure datasets is installed from main. Uncomment the following line if you face issues running this script:\n",
    "# !pip install git+https://github.com/huggingface/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "794aaced",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Audio, interleave_datasets, IterableDataset, load_dataset, SplitDict\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f210ca9a-486b-46a2-a675-2526a9bd83f5",
   "metadata": {},
   "source": [
    "### Define the dataset attributes\n",
    "\n",
    "The cell below is the original example including parameters for VoxPopuli and Mulitlingual LibriSpeech. These parameters may become handy if someone wants to inlcude these  datasets, so it\n",
    "is here commented out for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53344f3-c315-430a-a2f3-57aea6bb0e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \n",
    "#dataset_names = [\"mozilla-foundation/common_voice_11_0\", \"facebook/voxpopuli\", \"facebook/multilingual_librispeech\", \"google/fleurs\"]\n",
    "#dataset_config_names = [\"es\", \"es\", \"spanish\", \"es_419\"]\n",
    "#text_column_names = [\"sentence\", \"normalized_text\", \"text\", \"transcription\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc07293f-3ba4-4e89-a4ca-8e39409a8373",
   "metadata": {},
   "source": [
    "The example I use in this version shows how to combine the Common Voice 11 and FLEURS datasets for Greek (el). The modified version below produces interleaved datasets for both training and testing.\n",
    "\n",
    "-   The resulting training corpus will be equal to the sum of the individual datasets and we will use both test and validation splits for both. \n",
    "-   The test dataset will be equal to the **Common Voice 11 Test split only**. \n",
    "\n",
    "The parameters defined in the next cell are explained below. \n",
    "\n",
    "-   All parameters are in lists of the same length with one element per dataset.\n",
    "-   `dataset_names` contains the Hugging Face Hub namea of each of the datasets used\n",
    "-   `dataset_config_names` contains the respective language codes.  \n",
    "-   `text_column_names` contains the name used for the text feature (column) for each respective dataset.\n",
    "-   `train_splits` and `test_splits` contain split names used for the train and test interleaved datasets we will produce. If multiple splits need to be interleaved for any of the datasets, the respective split-names are concatenated into one string separated by the + sign. E.g to merge both train and validation the code should be `\"train+validation\"`. A special split name `\"-\"` (dash) can be used to suppress a dataset-split. This for example is the case for the test dataset which is based only on the Common Voice 11 test, and the fleurs test excluded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c53344f3-c315-430a-a2f3-57aea6bb0e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\"mozilla-foundation/common_voice_11_0\", \"google/fleurs\"]\n",
    "dataset_config_names = [\"el\", \"el_gr\"]\n",
    "text_column_names = [\"sentence\",  \"transcription\"]\n",
    "train_splits =[\"train+validation\",\"train+validation\"]\n",
    "test_splits  = [\"test\",\"-\"] # we want the test to come from one dataset only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215541f6-ee1c-4104-b43c-fa3f7fce0494",
   "metadata": {},
   "source": [
    "### Define the merging function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b722a48b-c576-4a63-b2a2-3c264890a75f",
   "metadata": {},
   "source": [
    "We define a function, `load_multiple_streaming_datasets`, that takes as argument a list of datasets, configs, splits (optional) and text column names (optional). It sets them to a specified sampling rate and interleaves them together, giving one merged dataset. This is all \n",
    "done in _streaming mode_: as we iterate over the merged dataset we load samples one-by-one on the fly. No data is\n",
    "saved to disk.\n",
    "\n",
    "We can also specify our strategy for interleaving datasets. The default strategy, `all_exhausted` is an oversampling \n",
    "strategy. In this case, the dataset construction is stopped as soon as every samples in every dataset \n",
    "has been added at least once. In practice, it means that if a dataset is exhausted, it will return to the \n",
    "beginning of this dataset until the stop criterion has been reached. You can specify `stopping_strategy=first_exhausted` \n",
    "for a subsampling strategy, i.e the dataset construction is stopped as soon one of the dataset runs out of samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61eb4cb1-ee27-4270-a474-1bb33e1df65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_multiple_streaming_datasets(\n",
    "    dataset_names: List,\n",
    "    dataset_config_names: List,\n",
    "    train_splits: Optional[List] = None,\n",
    "    test_splits: Optional[List] = None,\n",
    "    text_column_names: Optional[List] = None,\n",
    "    sampling_rate: Optional[int] = 16000,\n",
    "    stopping_strategy: Optional[str] = \"all_exhausted\",\n",
    "    **kwargs\n",
    ") -> IterableDataset:\n",
    "\n",
    "    if len(dataset_names) != len(dataset_config_names):\n",
    "        raise ValueError(\n",
    "            f\"Ensure one config is passed for each dataset, got {len(dataset_names)} datasets and\"\n",
    "            f\" {len(dataset_config_names)} configs.\"\n",
    "        )\n",
    "\n",
    "    if train_splits is not None and len(train_splits) != len(dataset_names):\n",
    "        raise ValueError(\n",
    "            f\"Ensure one train_split is passed for each dataset, got {len(dataset_names)} datasets and {len(train_splits)} splits.\"\n",
    "        )\n",
    "\n",
    "    if test_splits is not None and len(test_splits) != len(dataset_names):\n",
    "        raise ValueError(\n",
    "            f\"Ensure one test_split is passed for each dataset, got {len(dataset_names)} datasets and {len(test_splits)} splits.\"\n",
    "        )\n",
    "\n",
    "    if text_column_names is not None and len(text_column_names) != len(dataset_names):\n",
    "        raise ValueError(\n",
    "            f\"Ensure one text column name is passed for each dataset, got {len(dataset_names)} datasets and\"\n",
    "            f\" {len(text_column_names)} text column names.\"\n",
    "        )\n",
    "\n",
    "    train_splits = train_splits if train_splits is not None \\\n",
    "        else [\"train\" for i in range(len(dataset_names))]\n",
    "\n",
    "    test_splits = test_splits if test_splits is not None \\\n",
    "        else [\"test\" for i in range(len(dataset_names))]\n",
    "\n",
    "    text_column_names = (\n",
    "        text_column_names if text_column_names is not None \\\n",
    "            else [\"text\" for i in range(len(dataset_names))]\n",
    "    )\n",
    "\n",
    "\n",
    "    all_train_splits = []\n",
    "    all_test_splits  = []\n",
    "    # iterate over the datasets we want to interleave\n",
    "    for dset, cfgNm, trnSplit, tstSplit, colNm in zip(dataset_names,dataset_config_names,\\\n",
    "                                                train_splits,test_splits,text_column_names):\n",
    "\n",
    "        train_dset_splits = [load_dataset(dset, cfgNm, split=c, streaming=True, **kwargs) \\\n",
    "            for c in trnSplit.split('+') if c != '-']\n",
    "        test_dset_splits  = [load_dataset(dset, cfgNm, split=c, streaming=True, **kwargs) \\\n",
    "            for c in tstSplit.split('+') if c != '-']\n",
    "\n",
    "        train_dset_splits = [ds.cast_column(\"audio\", Audio(sampling_rate)) \\\n",
    "            for ds in train_dset_splits]\n",
    "        test_dset_splits  = [ds.cast_column(\"audio\", Audio(sampling_rate)) \\\n",
    "            for ds in test_dset_splits]\n",
    "\n",
    "        train_dset_splits = [ds.rename_column(colNm, \"text\") for ds in train_dset_splits]\n",
    "        test_dset_splits  = [ds.rename_column(colNm, \"text\") for ds in test_dset_splits]\n",
    "\n",
    "        cols2keep = set([\"audio\", \"text\"])\n",
    "\n",
    "        train_dset_splits = [ds.remove_columns(set(ds.features.keys()) - cols2keep) for ds in train_dset_splits]\n",
    "        test_dset_splits  = [ds.remove_columns(set(ds.features.keys()) - cols2keep) for ds in test_dset_splits]\n",
    "\n",
    "        all_train_splits +=   train_dset_splits\n",
    "        all_test_splits  +=   test_dset_splits\n",
    "        \n",
    "    interleaved_train_dataset = interleave_datasets(all_train_splits, stopping_strategy=stopping_strategy)\n",
    "    interleaved_test_dataset = interleave_datasets(all_test_splits, stopping_strategy=stopping_strategy)\n",
    "\n",
    "    return interleaved_train_dataset, interleaved_test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bc228b-ce9b-4cee-9092-1223ddfa51ad",
   "metadata": {},
   "source": [
    "Let's apply this function to load and merge our train and test the dataset dataset splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ae90f83-4ecd-46a3-98be-bd75706e0d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = load_multiple_streaming_datasets(dataset_names, \n",
    "        dataset_config_names=dataset_config_names, \n",
    "        train_splits = train_splits,\n",
    "        test_splits = test_splits,\n",
    "        text_column_names=text_column_names, \n",
    "        use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e31aa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train None\n",
      "test None\n"
     ]
    }
   ],
   "source": [
    "print('train',train_ds.dataset_size)\n",
    "print('test',train_ds.dataset_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6056a693-1fb0-45f4-ad43-be5f1812c1a5",
   "metadata": {},
   "source": [
    "### Iterate over the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffe011f-f905-4027-ab67-5c9c3b2b5ac0",
   "metadata": {},
   "source": [
    "We iterate over the dataset, loading and merging samples on the fly. Let's print the transcriptions for the first 10 samples of our merged dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75b3355a-3c06-4d23-af43-2b93b1ad70b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 1914it [00:00, 6148.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 πρόσταξε το Βασιλόπουλο.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 1701it [00:00, 6318.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Άλογο; Ο Τζοτζές έχει άλογο;\n",
      "2 στη βάση του βουνού αναφέρθηκε η παρουσία σκοτεινών συννέφων που δεν σχετίζονταν με ηφαιστιακή δραστηριότητα\n",
      "3 αυτά αποτελούν πλέον ξεχωριστές αρχές  οι οποίες εστιάζουν στην παροχή λύσεων σε πραγματικά προβλήματα της καθημερινότητας\n",
      "4 θυμήθηκα, σαν ήλθε η ώρα, τα λόγια μου.\n",
      "5 ρώτησε το πεύκο σκανδαλισμένο.\n",
      "6 συχνά είναι πιο αυτόνομοι από τα συμβατικά μέλη της ομάδας καθώς οι ομάδες τους μπορεί να συναντώνται σύμφωνα με διαφορετικές χρονικές ζώνες κάτι που μπορεί να μην είναι αποδεκτό από την τοπική τους διοίκηση\n",
      "7 το nhk δήλωσε επίσης ότι ο πυρηνικός σταθμός ηλεκτρικής ενέργειας κασιβαζάκι-καρίβα στον νομό νιιγκάτα λειτουργούσε κανονικά\n",
      "8 Ύστερα είπε\n",
      "9 Ώστε ο ήλιος είχε βασιλέψει\n",
      "10 υπάρχουν άπειρες πιθανές παραλλαγές αλλά αυτό συνεχίζουν να εννοούν οι περισσότεροι άνθρωποι όταν λένε «πάω στο ντίσνεϊ γουορλντ»\n",
      "11 η έρευνα στην τεχνητή νοημοσύνη περιλαμβάνει την κατασκευή μηχανών με σκοπό την αυτοματοποίηση εργασιών που απαιτούν νοήμονα συμπεριφορά\n",
      "12 Την έκλεισε σ' ένα σκαλισμένο, ασημένιο κουτί και την έδωσε στον αρχικαγκελάριο.\n",
      "13 και της τα έδωσε.\n",
      "14 το κτίριο φιλοξενούσε προσκυνητές που επισκέφτηκαν την ιερή πόλη την παραμονή του προσκυνήματος χατζ\n",
      "15 τα πρώτα κρούσματα της ασθένειας για φέτος καταγράφηκαν στα τέλη ιουλίου\n",
      "16 του κάκου τον γυρεύεις, Αφέντη μου!\n",
      "17 Τώρα έλα συ\n",
      "18 η χρήση εφαρμογών πλοήγησης gps στο τηλέφωνό σας μπορεί να γίνει ο πιο εύκολος και ο πιο πρακτικός τρόπος να πλοηγηθείτε όταν δεν βρίσκεστε στη χώρα σας\n",
      "19 οι περισσότερες περιοχές εξυπηρετούνται από μικρά ιαπωνικά λεωφορεία τύπου coaster τα οποία χαρακτηρίζονται από την άνεση και την ανθεκτικότητά τους\n",
      "20 και ήλθα να σε βρω.\n"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(train_ds):\n",
    "    print(i, sample[\"text\"])\n",
    "    if i == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d5ad08-b20e-4cba-a1a9-909fdbf030d4",
   "metadata": {},
   "source": [
    "The following code cell is lifted from the Whisper training notebook: https://github.com/huggingface/community-events/blob/main/whisper-fine-tuning-event/fine-tune-whisper-streaming.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed20e9cd-31c2-44cb-872b-333378a92fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "do_lower_case = False\n",
    "do_remove_punctuation = False\n",
    "\n",
    "normalizer = BasicTextNormalizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d13029-c24f-4a51-aff2-9251a2ceb4ce",
   "metadata": {},
   "source": [
    "Now we define a function to normalise our transcriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26e42417-4bd2-46f8-914e-3a6f9f3471ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_transcriptions(batch):\n",
    "    # optional pre-processing steps\n",
    "    transcription = batch[\"text\"]\n",
    "    if do_lower_case:\n",
    "        transcription = transcription.lower()\n",
    "    if do_remove_punctuation:\n",
    "        transcription = normalizer(transcription).strip()\n",
    "    batch[\"text\"] = transcription\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1c67fe-be4b-4ee5-9a1f-0d444f2b5c62",
   "metadata": {},
   "source": [
    "Let's apply the data pre-processing steps to our dataset and view the first 10 samples again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0babac71-9157-4d0f-a8a8-184547bdf501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 1914it [00:00, 9923.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 πρόσταξε το Βασιλόπουλο.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 1701it [00:00, 12544.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Άλογο; Ο Τζοτζές έχει άλογο;\n",
      "2 στη βάση του βουνού αναφέρθηκε η παρουσία σκοτεινών συννέφων που δεν σχετίζονταν με ηφαιστιακή δραστηριότητα\n",
      "3 αυτά αποτελούν πλέον ξεχωριστές αρχές  οι οποίες εστιάζουν στην παροχή λύσεων σε πραγματικά προβλήματα της καθημερινότητας\n",
      "4 θυμήθηκα, σαν ήλθε η ώρα, τα λόγια μου.\n",
      "5 ρώτησε το πεύκο σκανδαλισμένο.\n",
      "6 συχνά είναι πιο αυτόνομοι από τα συμβατικά μέλη της ομάδας καθώς οι ομάδες τους μπορεί να συναντώνται σύμφωνα με διαφορετικές χρονικές ζώνες κάτι που μπορεί να μην είναι αποδεκτό από την τοπική τους διοίκηση\n",
      "7 το nhk δήλωσε επίσης ότι ο πυρηνικός σταθμός ηλεκτρικής ενέργειας κασιβαζάκι-καρίβα στον νομό νιιγκάτα λειτουργούσε κανονικά\n",
      "8 Ύστερα είπε\n",
      "9 Ώστε ο ήλιος είχε βασιλέψει\n"
     ]
    }
   ],
   "source": [
    "ds = train_ds.map(normalize_transcriptions)\n",
    "\n",
    "for i, sample in enumerate(ds):\n",
    "    print(i, sample[\"text\"])\n",
    "    if i == 9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d135627a-a7aa-458c-94b8-57ddeae74a72",
   "metadata": {},
   "source": [
    "This time the transcriptions are in a consistent format. We can use this data to fine-tune our Whisper model. Note that since we've removed punctuation and casing, the Whisper model won't learn to predict these features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('whisper')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "56df8cc7a7ee396247f1aaa79087483e047b61a4716ebb1fc601848c69339422"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
